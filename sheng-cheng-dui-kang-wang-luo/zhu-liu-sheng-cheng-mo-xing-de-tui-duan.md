## （3）主流生成模型的推断

### （3.1）非线性独立主成分NICA

非线性独立主成分Nonlinear independent components analysis NIC 做明确定义的“连续”和“非线性”变换，如果已知隐变量z的概率分布，对于一个”连续、可微、可逆“的变换g，x由g\(z\)生成，那么生成变量x有如下的概率密度表达式：


$$
p_x(x)=p_z(g^{−1}(x))\left|det(\frac{∂g^{−1}(x)}{∂x})\right|
$$


从上面的式子中观察，如果密度函数 $$p_z $$ 是“可直接优化的Tractable”，并且 $$g$$ 逆变换的 $$Jacobian$$ 行列式是“可直接优化的Tractable”，那么密度函数 $$p_x$$ 也是Tractable的。换句话说，如果 $$g$$ 变换被精心设计，那么即便 $$z$$ 的分布相对简单，也会让生成变量 $$x$$ 具有一个相对复杂的分布。

非线性ICA模型的主要缺陷是它对 $$g$$ 的选择做了限制，比如其中的 $$z$$必须和x具有相同的维度，以满足 $$g$$ 的可逆性。如果要求变换 $$g$$ 的限制较少，比如允许$$z$$  有比 $$x$$ 更多的维度，由于 $$p_{model}$$ 无法明确表示，则为“隐式模型”，参考GANs。

### （3.2）完全观察置信网FVBs

完全观察置信网Fully visible belief networks FVBs 是一类模型用链式概率法则分解一个n维向量x的密度到一个一维概率密度乘机的形式：


$$
p_{model}(x)=∏_{i=1}^{n}p_{model}(x^{<i>}|x^{<1>},...,x^{<i−1>})
$$


FVBs是一系列 DeepMind 生成模型的基础，例如WaveNet。WaveNet能够生成真实的人类语音，它的缺陷在于一次只能生成一个样本，首先是$$x^{<1>}$$，下一个是$$x^{<2>}$$，时间复杂度是$$O(n)$$。WaveNet作为一种现代FVBNs变种，每一个$$x^{<i>}$$的分布都需要一个深度神经网络，因而每一步都需要复杂度较高的计算。此外，这些步骤没有办法并行化，它耗费两分钟的计算来生成一秒钟的音频，因而无法用于交互式对话。GANs被设计并行化地生成所有的 $$x^{<i>}$$，有着更快的解码速度。

**IMAGE3.2.1**

### （3.3）变分自编码VAE

解决一些具有明确概率密度表示，但不具有“直接优化的tractable”的密度函数\(如，隐变量模型不能直接凸优化求解\)，则需要使用近似推断方法。一般地讲，近似推断分为两大类，一类是“确定性近似”如“变分推断Variational Methods”，另一类是“随机近似Stochastic Approximations”，如“马尔科夫链蒙特卡洛方法Markov Chain Monte Carlo Methods”。

变分自编码Variational AutoEncoder使用近似推断技术，VAE构建的生成模型具有如下示意图：

**IMAGE3.3.1**

VAE选择多元高斯分布作为输出概率，生成变量x具有概率密度：


$$
p(x|z;θ)=N(x|f(z;θ),σ^2∗I)
$$


其中生成函数 $$f(z;θ)$$ 接受一个多元高斯随机变量，确定了 $$x$$ 的均值，$$σ$$ 是超参数确定了一个对角协方差矩阵。通过积分掉 $$z$$，推断其中的参数 $$\theta$$ 来最大化似然


$$
∏_{i=1}^{m}p_{model}(x(i)|θ)=∫∏_{i=1}^{m}p_{model}(x(i)|z;θ)p_{prior}(z)dz
$$


这就进入了标准的隐变量模型推断框架，通常使用EM算法，将变量$$z$$ 看作是和 $$x$$ 绑定的不完全观察变量。但事实上，由于$$f(z;θ)$$是深度生成神经网络，后验概率密度函数$$P_{posterior}(z|{x_{1},...x_{m}})$$$$$$则很难以某种形式给出。一种 “近似推断” 的方法叫 “变分贝叶斯Variational Bayes”，它构造 $$z$$ 的一个近似的参数化分布 $$Q$$ 作为 $$z$$ 的后验分布近似。

我们记$$P_{model}(X)=∏_{i=1}^{m}p_{model}(x(i)|θ)$$ 还有$$P_{model}(X|z)=∏_{i=1}^{m}p_{model}(x(i)|z;θ)$$，则有似然表达式：


$$
P_{model}(X)=∫P_{model}(X|z)p_{prior}(z)dz
$$


似然函数满足下述等式：


$$
log P_{model}(X)−D_{KL}\left(Q(z|X)∥P_{posterior}(z|X)\right)=E_{z∼Q}[log P_{model}(X|z)]−D_{KL}(Q(z|X)∥p_{prior}(z))
$$


式子的右边是 $$log$$ 似然$$log P_{model}(X)$$的下界。

变分EM最大化下界的过程，通常有两步：

（1）E-step 对下界展开去掉隐变量 $$z$$，即对包含 $$z$$ 的统计量进行期望替换为包含 $$Q$$  参数的函数。

（2）M-step 让下界最大化，$$Q$$  与 $$P$$  的参数分块坐标上升。

在 

