## （2）最大化似然密度估计

这一节，我们通过与其他生成模型的比较，来看GANs是如何工作的。我们首先关注“最大似然估计MLE”准则下的生成模型，一些生成模型不直接优化最大似然函数，但却等效于最大似然估计，GANs就属于这一类。

### （2.1）最大化似然与KL熵

这一节，我们通过与其他生成模型的比较，来看GANs是如何工作的。我们首先关注“最大似然估计MLE”准则下的生成模型，一些生成模型不直接优化最大似然函数，但却等效于最大似然估计，GANs就属于这一类。

**IMAGE2.1.1**

回顾最大似然估计，它定义一个由参数 $$\theta$$ 确定的概率密度函数，对于一个包含m个样本的训练数据集，我们有似然函数$$\prod_{i=1}^{m} p_{model}(x^{(i)};\theta)$$ , 最大似然准则是找到参数 $$\theta$$ 使似然函数最大化。为了避免数值下溢，我们通常转换成$$log\ sum$$的形式：


$$
\theta^{*}=argmax_{\theta} \prod_{i=1}^{m} p_{model}(x^{(i)}\ |\ \theta)
=argmax_{\theta} log\prod_{i=1}^{m} p_{model}(x^{(i)}\ |\ \theta)=argmax_{\theta}\sum_{i=1}^{m}log\ p_{model}(x^{(i)}\ |\ \theta)
$$


如果我们知道样本分布 $$p_{model}$$ 的具体形式，最小化“样本分布 $$p_{data}$$”与“模型分布 $$p_{model}$$”之间的KL损失，就可以得到一个近似分布$$p_{model}$$，写成表达式如下：


$$
\theta^{*}=argmin_{\theta}D_{KL}(p_{data}(x)\ \|\ p_{model}(x\ |\ \theta))
$$


在实践中，我们无法获得 $$p_{data}$$的具体形式，只能观察到m个经验样本。如果$$p_{data}$$被替换为m个点构成的经验分布，即每经验样本点的概率密度为$$\frac{1}{m}$$而其他点为0，则最小化KL散度等价于最大似然估计：


$$
\theta^{*}=argmax_{\theta}\mathbb{E}_{x\sim p_{data}}log\ p_{model}(x\ |\ \theta)
$$


### （2.2）推断方法总结

下图是深度生成模型最大似然估计的树状图：

**IMAGE2.2.1**

广义范围内，深度生成模型的特性是，通过输入高斯随机噪音或包含预设条件变量来生成相应的随机样本。求解最大似然深度生成模型的方法，可以划分为“显式模型Explicit Model” 与 “隐式模型Implicit Model”两大类：

（2.2.1）显式模型，优化明确的 $$p_{model}$$ 似然，这种情况又可以分为“直接优化的Tractable“和”近似推断Approximate“两类，前者常见如PixelRNN、ICA、NADE，后者RBM、VAE。

（2.2.2）隐式模型，不直接优化 $$p_{model}$$ ，特别当 $$p_{model}$$ 无法参数化表示的情况下，这一类深度生成模型的方法分为两种，一种是模型随机变换一个存在的样本，从而获得另一个具有相同概率密度函数的随机样本，如GSN。另一种，则是不需要任何初始输入就可以产生，如基本的GAN只需要随机噪音。

无论对”隐式模型“还是”显式模型“，GAN只基于生成器采样方式进行训练，不与隐含的 $$p_{model}$$ 直接交互，利用判别器度量“观察样本空间”与“生成样本空间”之间的距离，这决定了GAN是一个通用性很强的学习框架。

