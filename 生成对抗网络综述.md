# 生成对抗网络GAN综述

## （1）为什么研究生成模型？——生成对抗网络的应用简述

通常来说，生成模型提供一个生成函数而非概率密度函数，训练并从生成模型中采样是对高维随机变量建模的一种有效方法，高维概率分布的刻画是应用数学和工程领域非常重要的研究课题。

### （1.1）强化学习

生成模型引入到强化学习机制中。通常情况下，强化学习可以分为model-based（Policy Gradient）与model-free（Q learning）两种，而model-based算法就需要学习一个生成模型。

（1.1.1） 基于state和action到下一个state的时间序列样本序列，GAN可以学习到一个给定state和action下未来状态的条件概率分布。给定初始state和对action的时序随机采样，则可以确定未来可能state与预期state之间的差别。

（1.1.2） 我们也可以用GAN模拟强化学习交互的环境，提供虚拟反馈，并利用生成模型指导state和action的搜索路径。

（1.1.3） GAN与Actor-Critic联系、与逆强化学习、模仿学习的结合。

### （1.2）半监督学习

生成模型可以在样本数据缺失情况下训练，提供对缺失样本的预测，这方面，一个典型的应用是半监督学习semi-supervised learning。在半监督问题中，大量的样本标签是缺失的，GAN在半监督学习中表现出与主流方法相当的效果。

### （1.3）图像与视觉应用

（1.3.1）生成模型和GANs使机器学习模型可以给出“多形式（模态）”输出，例如预测视频的下一帧。

**IMAGE1.3.1**

在很多的应用中，我们需要从分布中确切地生成样本来使用，这包括：

（1.3.2）图片的超分辨率转换，这类任务通常需要从低分辨率图像合成出一张高分辨率图像。我们需要的高分辨率样本是低分辨率输入条件下的多种可能样本中的一个，这时候生成模型更适合完成这个任务。

**IMAGE1.3.2**

（1.3.3）做艺术创作，通过不断交互的方式帮助用户创作图像，这类应用中只需要用户提供还在构想中的一些粗糙轮廓就能生成出具有真实感的图像。

**IMAGE1.3.3**

（1.3.4）图像到图像转译，例如将航空图片转为地图，或者将素描变成图片。

**IMAGE1.3.4**

## （2）最大化似然密度估计框架下深度生成模型的比较

这一节，我们通过与其他生成模型的比较，来看GANs是如何工作的。我们首先关注“最大似然估计MLE”准则下的生成模型，一些生成模型不直接优化最大似然函数，但却等效于最大似然估计，GANs就属于这一类。

### （2.1）最大化似然与深度生成模型的推断方式

这一节，我们通过与其他生成模型的比较，来看GANs是如何工作的。我们首先关注“最大似然估计MLE”准则下的生成模型，一些生成模型不直接优化最大似然函数，但却等效于最大似然估计，GANs就属于这一类。

**IMAGE2.1.1**

回顾最大似然估计，它定义一个由参数 $$\theta$$ 确定的概率密度函数，对于一个包含m个样本的训练数据集，我们有似然函数$$\prod_{i=1}^{m} p_{model}(x^{(i)};\theta)$$ , 最大似然准则是找到参数 $$\theta$$ 使似然函数最大化。为了避免数值下溢，我们通常转换成$$log\ sum$$的形式：


$$
\theta^{*}=argmax_{\theta} \prod_{i=1}^{m} p_{model}(x^{(i)};\theta)
=argmax_{\theta} log\prod_{i=1}^{m} p_{model}=argmax_{\theta}\sum_{i=1}^{m}log\ p_{model}(x^{(i)};\theta)
$$


如果我们知道样本分布 $$p_{model}$$ 的具体形式，最小化“样本分布 $$p_{data}$$”与“模型分布 $$p_{model}$$”之间的KL损失，就可以得到一个近似分布$$p_{model}$$，写成表达式如下：


$$
\theta^{*}=argmin_{\theta}D_{KL}(p_{data}(x)\|p_{model}(x;\theta))
$$


在实践中，我们无法获得 $$p_{data}$$的具体形式，只能观察到m个经验样本。如果$$p_{data}$$被替换为m个点构成的经验分布，即每经验样本点的概率密度为$$\frac{1}{m}$$而其他点为0，则最小化KL散度等价于最大似然估计。


$$
\theta^{*}=argmax_{\theta}\mathbb{E}_{x\sim p_{data}}log\ p_{model}(x;\theta)
$$


下图是深度生成模型最大似然估计的树状图：

**IMAGE2.1.2**

广义范围内，深度生成模型的特性是，通过输入高斯随机噪音或包含预设条件变量来生成相应的随机样本。求解最大似然深度生成模型的方法，可以划分为“显式模型Explicit Model” 与 “隐式模型Implicit Model”两大类：

（1）显式模型，优化明确的Pmodel似然，这种情况又可以分为“直接优化的Tractable“和”近似推断Approximate“两类，前者常见如PixelRNN、ICA、NADE，后者RBM、VAE。

（2）隐式模型，不直接优化Pmodel，特别当Pmodel无法参数化表示的情况下，这一类深度生成模型的方法分为两种，一种是模型随机变换一个存在的样本，从而获得另一个具有相同概率密度函数的随机样本，如GSN。另一种，则是不需要任何初始输入就可以产生，如基本的GAN只需要随机噪音。

无论对”隐式模型“还是”显式模型“，GAN只基于生成器采样方式进行训练，不与隐含Pmodel直接交互，利用判别器度量“观察样本空间”与“生成样本空间”之间的距离，这决定了GAN是一个通用性很强的学习框架。

