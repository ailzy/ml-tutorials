# 生成对抗网络GAN

## （1）生成对抗网络的应用

通常来说，生成模型提供一个生成函数而非概率密度函数，训练并从生成模型中采样是对高维随机变量建模的一种有效方法，高维概率分布的刻画是应用数学和工程领域非常重要的研究课题。

### （1.1）强化学习

生成模型引入到强化学习机制中。通常情况下，强化学习可以分为model-based（Policy Gradient）与model-free（Q learning）两种，而model-based算法就需要学习一个生成模型。

（1.1.1） 基于state和action到下一个state的时间序列样本序列，GAN可以学习到一个给定state和action下未来状态的条件概率分布。给定初始state和对action的时序随机采样，则可以确定未来可能state与预期state之间的差别。

（1.1.2） 我们也可以用GAN模拟强化学习交互的环境，提供虚拟反馈，并利用生成模型指导state和action的搜索路径。

（1.1.3） GAN与Actor-Critic联系、与逆强化学习、模仿学习的结合。

### （1.2）半监督学习

生成模型可以在样本数据缺失情况下训练，提供对缺失样本的预测，这方面，一个典型的应用是半监督学习semi-supervised learning。在半监督问题中，大量的样本标签是缺失的，GAN在半监督学习中表现出与主流方法相当的效果。

### （1.3）图像与视觉应用

（1.3.1）生成模型和GANs使机器学习模型可以给出“多形式（模态）”输出，例如预测视频的下一帧。

**IMAGE1.3.1**

在很多的应用中，我们需要从分布中确切地生成样本来使用，这包括：

（1.3.2）图片的超分辨率转换，这类任务通常需要从低分辨率图像合成出一张高分辨率图像。我们需要的高分辨率样本是低分辨率输入条件下的多种可能样本中的一个，这时候生成模型更适合完成这个任务。

**IMAGE1.3.2**

（1.3.3）做艺术创作，通过不断交互的方式帮助用户创作图像，这类应用中只需要用户提供还在构想中的一些粗糙轮廓就能生成出具有真实感的图像。

**IMAGE1.3.3**

（1.3.4）图像到图像转译，例如将航空图片转为地图，或者将素描变成图片。

**IMAGE1.3.4**

## （2）最大化似然密度估计

这一节，我们通过与其他生成模型的比较，来看GANs是如何工作的。我们首先关注“最大似然估计MLE”准则下的生成模型，一些生成模型不直接优化最大似然函数，但却等效于最大似然估计，GANs就属于这一类。

### （2.1）最大化似然与KL熵

这一节，我们通过与其他生成模型的比较，来看GANs是如何工作的。我们首先关注“最大似然估计MLE”准则下的生成模型，一些生成模型不直接优化最大似然函数，但却等效于最大似然估计，GANs就属于这一类。

**IMAGE2.1.1**

回顾最大似然估计，它定义一个由参数 $$\theta$$ 确定的概率密度函数，对于一个包含m个样本的训练数据集，我们有似然函数$$\prod_{i=1}^{m} p_{model}(x^{(i)};\theta)$$ , 最大似然准则是找到参数 $$\theta$$ 使似然函数最大化。为了避免数值下溢，我们通常转换成$$log\ sum$$的形式：


$$
\theta^{*}=argmax_{\theta} \prod_{i=1}^{m} p_{model}(x^{(i)}\ |\ \theta)
=argmax_{\theta} log\prod_{i=1}^{m} p_{model}(x^{(i)}\ |\ \theta)=argmax_{\theta}\sum_{i=1}^{m}log\ p_{model}(x^{(i)}\ |\ \theta)
$$


如果我们知道样本分布 $$p_{model}$$ 的具体形式，最小化“样本分布 $$p_{data}$$”与“模型分布 $$p_{model}$$”之间的KL损失，就可以得到一个近似分布$$p_{model}$$，写成表达式如下：


$$
\theta^{*}=argmin_{\theta}D_{KL}(p_{data}(x)\ \|\ p_{model}(x\ |\ \theta))
$$


在实践中，我们无法获得 $$p_{data}$$的具体形式，只能观察到m个经验样本。如果$$p_{data}$$被替换为m个点构成的经验分布，即每经验样本点的概率密度为$$\frac{1}{m}$$而其他点为0，则最小化KL散度等价于最大似然估计。


$$
\theta^{*}=argmax_{\theta}\mathbb{E}_{x\sim p_{data}}log\ p_{model}(x\ |\ \theta)
$$


### （2.2）推断方法总结

下图是深度生成模型最大似然估计的树状图：

**IMAGE2.2.1**

广义范围内，深度生成模型的特性是，通过输入高斯随机噪音或包含预设条件变量来生成相应的随机样本。求解最大似然深度生成模型的方法，可以划分为“显式模型Explicit Model” 与 “隐式模型Implicit Model”两大类：

（2.2.1）显式模型，优化明确的Pmodel似然，这种情况又可以分为“直接优化的Tractable“和”近似推断Approximate“两类，前者常见如PixelRNN、ICA、NADE，后者RBM、VAE。

（2.2.2）隐式模型，不直接优化Pmodel，特别当Pmodel无法参数化表示的情况下，这一类深度生成模型的方法分为两种，一种是模型随机变换一个存在的样本，从而获得另一个具有相同概率密度函数的随机样本，如GSN。另一种，则是不需要任何初始输入就可以产生，如基本的GAN只需要随机噪音。

无论对”隐式模型“还是”显式模型“，GAN只基于生成器采样方式进行训练，不与隐含Pmodel直接交互，利用判别器度量“观察样本空间”与“生成样本空间”之间的距离，这决定了GAN是一个通用性很强的学习框架。

## （3）主流生成模型的推断

### （3.1）非线性独立主成分NICA

非线性独立主成分Nonlinear independent components analysis NIC 做明确定义的“连续”和“非线性”变换，如果已知隐变量$$z$$ 的概率分布，对于一个”连续、可微、可逆“的变换 $$g$$，$$x$$ 由 $$g(z)$$ 生成，那么生成变量 $$x$$ 有如下的概率密度表达式：


$$
p_x(x) = p_z(g^{-1}(x))\left|\det(\frac{\partial g^{-1}(x)}{\partial x})\right|
$$


从上面的式子中观察，如果密度函数 $$p_{z}$$ 是“可直接优化的Tractable”，并且 $$g$$ 逆变换的 $$Jacobian$$ 行列式是“可直接优化的Tractable”，那么密度函数 $$p_{x}$$ 也是Tractable的。换句话说，如果 $$g$$ 变换被精心设计，那么即便 $$z$$ 的分布相对简单，也会让生成变量 $$x$$ 具有一个相对复杂的分布。

非线性ICA模型的主要缺陷是它对 $$g$$ 的选择做了限制，比如其中的 $$z$$ 必须和 $$x$$ 具有相同的维度，以满足$$g$$ 的可逆性。如果要求变换 $$g$$ 的限制较少，比如允许 $$z$$ 有比 $$x$$ 更多的维度，由于 $$p_{model}$$ 无法明确表示则为“隐式模型”，参考GANs。

### （3.2）完全观察置信网FVBs

完全观察置信网Fully visible belief networks FVBs 是一类模型用链式概率法则分解一个n维向量 $$x$$ 的密度到一个一维概率密度乘机的形式：


$$
p_{model}({x}) = \prod_{i=1}^{n} (
{x}^{<i>}\ |\ {x}^{<1>},...,{x}^{<i-1>})
$$


FVBs是一系列DeepMind生成模型的基础，例如WaveNet。WaveNet能够生成真实的人类语音，它的缺陷在于一次只能生成一个样本，首先是$$x^{<1>}$$，下一个是$$x^{<2>}$$，时间复杂度是$$O(n)$$。WaveNet作为一种现代FVBNs变种，每一个 $${x}^{<i>}$$ 的分布都需要一个深度神经网络，因而每一步都需要复杂度较高的计算。此外，这些步骤没有办法并行化，它耗费两分钟的计算来生成一秒钟的音频，因而无法用于交互式对话。GANs被设计并行化地生成所有的$${x}^{<i>}$$，有着更快的解码速度。

**IMAGE3.2.1**

### （3.3）变分自编码VAE

解决一些具有明确概率密度表示，但不具有“直接优化的tractable”的密度函数\(如，隐变量模型不能直接凸优化求解\)，则需要使用近似推断方法。一般地讲，近似推断分为两大类，一类是“确定性近似”如“变分推断Variational Methods”，另一类是“随机近似Stochastic Approximations”，如“马尔科夫链蒙特卡洛方法Markov Chain Monte Carlo Methods”。

变分自编码Variational AutoEncoder使用近似推断技术，VAE构建的生成模型具有如下示意图：

**IMAGE3.2.2**

VAE选择多元高斯分布作为输出概率，生成变量 $$x$$ 具有概率密度：


$$
p(x|z;\theta)= \mathcal{N}(x\ |\ f(z;\theta), \sigma^2*I)
$$


其中生成函数 $$f(z;\theta)$$ 确定了$$x$$ 的均值，$$\sigma$$ 是超参数确定了一个对角协方差矩阵。通过积分掉 $$z$$，推断其中的参数 $$\theta$$ 来最大化似然


$$
\prod_{i=1}^{m}p_{model}(x^{(i)}\ |\ \theta) = \int \prod_{i=1}^{m}p_{model}(x^{(i)}\ |\ z;\theta)p(z)dz
$$
我们记$$P_{model}({\bf{x}})=\prod_{i=1}^{m}p_{model}(x^{(i)}\ |\ \theta)$$，还有

这就进入了标准的隐变量模型推断框架，通常使用EM算法，将变量 $$z$$ 看作是和 $$x$$ 绑定的不完全观察变量。但事实上，由于$$f(z;\theta)$$是深度生成神经网络，后验概率密度函数则很难以某种形式给出。一种 “近似推断” 的方法叫 “变分贝叶斯Variational Bayes”，它构造$$z$$   
的一个近似的参数化分布$$Q$$，似然函数得到最大化似然的近似下界:

$$$$

![](https://aimind.atlassian.net/wiki/download/thumbnails/12222465/image2018-5-17_15-50-9.png?version=1&modificationDate=1526543413588&cacheVersion=1&api=v2&width=728&height=63)

![](https://aimind.atlassian.net/wiki/download/thumbnails/12222465/image2018-5-17_16-55-25.png?version=1&modificationDate=1526547331324&cacheVersion=1&api=v2&width=396&height=367)

选择Q

\(

z

\|

X

\)以

μ

\(

X

\)作为均值

Σ

\(

X

\)作为协方差的高斯分布

，最大化下界需要同时对 θ和分布Q的参数。VAE的推断过程，将式子中的Q分布视为编码器，P分布视为解码器。

